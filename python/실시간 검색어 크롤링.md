# 실시간 검색어 크롤링

### 함수와 모듈

---

- 함수 : 조립하는 기계
- 모듈 : 조립 기계 키트 (함수 모아놓은)

`모듈명.함수이름(매개변수)`

### get 함수

---

: return 응답값을 주는 함수

### requests 모듈

---

`모듈명.함수이름(매개변수)` = requests.get(url주소)

- requests.get : GET 요청을 보내는 기능
- 요청 함수 - PUT, GET, POST, DELETE
- 클라이언트-요청 / 서버-응답
- 응답은 requests에서 reponse라는 통에 담겨 옴

### request

---

- response 200 : 요청이 성공했다

### BeautifulSoup(데이터, 파싱방법)

---

: 어떤 통에다가 정보를 담아주는 기능

`from bs4 import BeautifulSoup`

- bs4 모듈 안에 있는 BeautifulSoup함수만 쓸게요
- **Parsing** : 문자열을 분석해서 의미있는 데이터로 변환
- **html.parser** : html의 parsing기능을 할 수 있는 parser 이용

```python
import requests
from bs4 import BeautifulSoup

url = "http://www.daum.net/"
response = requests.get(url)

#499번째 인덱스까지 
print(response.text[:500])

#response.text를 html의 parser을 이용
soup = BeautifulSoup(response.text, 'html.parser')

print(soup.title) 
-><title>제목</title>
print(soup.title.string)
->제목
print(soup.span)
->첫 번째 span태그
print(soup.findAll('span'))
->모든 span태그
```

- 새 html 파일에 크롤링한 정보 넣기

```python
from bs4 import BeautifulSoup
import requests

url = "http://www.daum.net/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

#daum.html이라는 파일 생성, 열기
file = open("daum.html","w")
#response.text를 daum.html에 쓰기
file.write(response.text)
#daum.html 닫기
file.close()

print(soup.title)
print(soup.title.string)
print(soup.span)
print(soup.findAll('span'))
```

- 해당 태그, 해당 클래스를 가진 코드만 크롤링

```python
from bs4 import BeautifulSoup
import requests

url = "http://www.daum.net/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# html 문서에서 모든 a태그 중 link_favors라는 클래스를 가진 코드를 가져옴
print(soup.findAll("a","link_favors"))
```

- 이쁘게 정렬
- **파일로 출력하기 - open(파일, 모드)**
    
    모드:
    
     r-read(읽기전용) / w-write(덮어쓰기) / a-append(기존 내용 보존 + 내용 붙이기)
    

```python
from bs4 import BeautifulSoup
import requests
from datetime import datetime

url = "http://www.daum.net/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
rank = 1

#모든 a태그 중 link_favorsh 클래스 인 것
results = soup.findAll('a','link_favorsch')

#파일로 출력하기
search_rank_file = open("rankresult.txt","w")

#오늘 날짜 출력
print(datetime.today().strftime("%Y년 %m월 %d일의 실시간 검색어 순위입니다."))

#text만 추출, 줄 바꿈해서 출력
for result in results:
		search_rank_file.write(str(rank)+result.get_text())
    print(rank,result.get_text(),"\n")
	  #번호 매기기
		rank += 1
```